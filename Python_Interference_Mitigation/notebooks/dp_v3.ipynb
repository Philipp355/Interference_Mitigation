{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import mat73\n",
    "from torchsummary import summary\n",
    "\n",
    "from src.models.ref_dual_path_autoencoder import RefDualPathAutoencoder\n",
    "from src.training.train_ref_dp_model import train_refdual_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\WangCo\\Documents\\ProgramData\\Python\\radar_interference_suppression\\notebooks\n",
      "Project root directory: c:\\Users\\WangCo\\Documents\\ProgramData\\Python\\radar_interference_suppression\n",
      "Python path: ['c:\\\\Users\\\\WangCo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python311.zip', 'c:\\\\Users\\\\WangCo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\DLLs', 'c:\\\\Users\\\\WangCo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib', 'c:\\\\Users\\\\WangCo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311', '', 'C:\\\\Users\\\\WangCo\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\WangCo\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\WangCo\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\WangCo\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\WangCo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\WangCo\\\\Documents\\\\ProgramData\\\\Python\\\\radar_interference_suppression']\n",
      "'src' directory exists: True\n",
      "'datasets' directory exists: True\n",
      "'notebooks' directory exists: True\n",
      "'outputs' directory exists: True\n",
      "'configs' directory exists: True\n",
      "\n",
      "New working directory: c:\\Users\\WangCo\\Documents\\ProgramData\\Python\\radar_interference_suppression\n"
     ]
    }
   ],
   "source": [
    "# Set absolute path for the project root directory\n",
    "PROJECT_ROOT = r'c:\\Users\\WangCo\\Documents\\ProgramData\\Python\\radar_interference_suppression'\n",
    "\n",
    "# Ensure the project root directory is in the Python path\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Verify the path settings\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Project root directory:\", PROJECT_ROOT)\n",
    "print(\"Python path:\", sys.path)\n",
    "\n",
    "# Check if directories exist\n",
    "expected_dirs = ['src', 'datasets', 'notebooks', 'outputs', 'configs']\n",
    "for dir_name in expected_dirs:\n",
    "    dir_path = os.path.join(PROJECT_ROOT, dir_name)\n",
    "    print(f\"'{dir_name}' directory exists:\", os.path.exists(dir_path))\n",
    "\n",
    "# Change working directory to project root\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(\"\\nNew working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "# import src\n",
    "from src.utils import analysis\n",
    "from src.utils import training_validation\n",
    "from src.utils import evaluation\n",
    "from src.data import data_utils\n",
    "from src.utils import visualization\n",
    "from src.data.dataset import RadarDataset\n",
    "from src.models import baseline_tec\n",
    "from src.models import ref_dual_path_autoencoder\n",
    "from src.training import train_ref_model\n",
    "from src.utils import evaluation_ref_dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules reloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# # # Reload the modules to get the latest changes\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# # # # Reload the modules to get the latest changes\n",
    "# importlib.reload(src)\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(visualization)\n",
    "importlib.reload(analysis)\n",
    "importlib.reload(training_validation)\n",
    "importlib.reload(evaluation)\n",
    "importlib.reload(evaluation_ref_dp)\n",
    "importlib.reload(ref_dual_path_autoencoder)\n",
    "importlib.reload(train_ref_model)\n",
    "\n",
    "print(\"Modules reloaded successfully!\")\n",
    "# # # Reload the modules to get the latest changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scenario data...\n",
      "\n",
      "Scenario 1 loaded:\n",
      "Shape: (256, 128, 60)\n",
      "Number of frames: 60\n",
      "[{'scenario_id': 1, 'num_frames': 60, 'shape': (256, 128, 60)}]\n",
      "Input shapes - radar_cube: (256, 128, 60), clean_data: (256, 128, 60)\n",
      "Data types - radar_cube: complex128, clean_data: complex128\n",
      "\n",
      "Data statistics:\n",
      "Radar cube - Mean amplitude: 0.132\n",
      "Radar cube - Max amplitude: 0.247\n",
      "Clean data - Mean amplitude: 0.129\n",
      "Clean data - Max amplitude: 0.198\n",
      "\n",
      "Data Analysis Summary:\n",
      "Total number of scenarios: 1\n",
      "Total number of frames: 60\n",
      "Data dimensions: (256, 128, 60)\n",
      "\n",
      "Scenario 1:\n",
      "Frames: 60\n",
      "Input SIR: 12.91 dB\n"
     ]
    }
   ],
   "source": [
    "scenario_files = [\n",
    "    os.path.join(PROJECT_ROOT, 'datasets', 'raw', 'test_dataset_2t1_60f.mat')  \n",
    "]\n",
    "\n",
    "# Load and analyze all scenarios\n",
    "print(\"Loading scenario data...\")\n",
    "radar_cube, clean_data, scenario_info = data_utils.load_multiple_scenarios(scenario_files)\n",
    "\n",
    "# DEBUG INFO\n",
    "print(f\"Input shapes - radar_cube: {radar_cube.shape}, clean_data: {clean_data.shape}\")\n",
    "print(f\"Data types - radar_cube: {radar_cube.dtype}, clean_data: {clean_data.dtype}\")\n",
    "\n",
    "# Check data values\n",
    "print(\"\\nData statistics:\")\n",
    "print(f\"Radar cube - Mean amplitude: {np.mean(np.abs(radar_cube)):.3f}\")\n",
    "print(f\"Radar cube - Max amplitude: {np.max(np.abs(radar_cube)):.3f}\")\n",
    "print(f\"Clean data - Mean amplitude: {np.mean(np.abs(clean_data)):.3f}\")\n",
    "print(f\"Clean data - Max amplitude: {np.max(np.abs(clean_data)):.3f}\")\n",
    "\n",
    "analysis.analyze_scenario_data(radar_cube, clean_data, scenario_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze original data\n",
    "print(\"Analyzing original data...\")\n",
    "analysis.analyze_rd_processing(radar_cube, clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex_to_real() triggered:\n",
      "Real part range: [-0.246, 0.246]\n",
      "Imag part range: [-0.246, 0.246]\n",
      "complex_to_real() triggered:\n",
      "Real part range: [-0.198, 0.198]\n",
      "Imag part range: [-0.198, 0.198]\n",
      "Dataset shapes after processing:\n",
      "Interference data shape: (2, 256, 128, 60)\n",
      "Clean data shape: (2, 256, 128, 60)\n",
      "Number of frames: 60\n",
      "size of validation: 12\n",
      "Initializing model......\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' For dual-path autoencoder usage '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preparation\n",
    "full_dataset = RadarDataset(radar_cube, clean_data)\n",
    "val_size = int(len(full_dataset) * 0.2)\n",
    "print(f\"size of validation: {val_size}\")\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training preparation\n",
    "\"\"\"\"\"\"\"\"\" For dual-path autoencoder usage \"\"\"\"\"\"\"\"\"\n",
    "# Initialize model\n",
    "print(\"Initializing model......\")\n",
    "num_samples, num_chirps = radar_cube.shape[0:2]\n",
    "# model = dualpath_autoencoder.DualPathRadarAutoencoder(num_samples=num_samples, num_chirps=num_chirps)\n",
    "model = ref_dual_path_autoencoder.RefDualPathAutoencoder()\n",
    "\"\"\"\"\"\"\"\"\" For dual-path autoencoder usage \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "# # Create dummy inputs for both paths\n",
    "# batch_size = 4\n",
    "# summary_size = [\n",
    "#     (batch_size, 2, 256, 128),  # Raw signal input: [batch, channels, samples, chirps]\n",
    "#     (batch_size, 2, 256, 128)   # RD map input: [batch, channels, samples, chirps]\n",
    "# ]\n",
    "# print(f\"Summary input shapes: {summary_size}\")\n",
    "# summary(model, summary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting model training...\")\n",
    "trained_model, history, memory_usage, training_time = train_refdual_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "visualization.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(evaluation)\n",
    "# Evaluate model, get training results\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "for idx, info in enumerate(scenario_info):\n",
    "    start_frame = sum(s['num_frames'] for s in scenario_info[:idx])\n",
    "    scenario_radar = radar_cube[:, :, start_frame:start_frame + info['num_frames']]\n",
    "    scenario_clean = clean_data[:, :, start_frame:start_frame + info['num_frames']]\n",
    "\n",
    "    print(f\"\\nEvaluating Scenario {info['scenario_id']}:\")\n",
    "    # train_results = evaluate_reconstruction(trained_model, scenario_radar, scenario_clean)\n",
    "\n",
    "    \"\"\"\"\"\"\"\"\" For dual-path autoencoder usage \"\"\"\"\"\"\"\"\"\n",
    "    # After training\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    train_results, reconstructed_data = evaluation_ref_dp.evaluate_refdual_reconstruction(\n",
    "    trained_model, \n",
    "    scenario_radar, \n",
    "    scenario_clean,\n",
    "    frame_idx=0\n",
    ")\n",
    "    \"\"\"\"\"\"\"\"\" For dual-path autoencoder usage \"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    # reconstructed_cube, train_results = evaluate_and_save_results(\n",
    "    #     trained_model,\n",
    "    #     scenario_radar,\n",
    "    #     scenario_clean\n",
    "    # )\n",
    "\n",
    "# Store original metrics\n",
    "train_ref_metrics_2t1 = train_results\n",
    "# print(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ref_dual_path_autoencoder.RefDualPathAutoencoder()\n",
    "\n",
    "\n",
    "model.count_parameters()\n",
    "\n",
    "\n",
    "time_input = torch.randn(8, 2, 256, 128)  \n",
    "rd_input = torch.randn(8, 2, 256, 128)    \n",
    "\n",
    "\n",
    "output = model(time_input, rd_input)\n",
    "\n",
    "\n",
    "print(f\"output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
